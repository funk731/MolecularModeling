{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eneskelestemur/MolecularModeling/blob/main/Labs/lab05_machine_learning/Machine_learning.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages that need to be installed\n",
    "%pip install rdkit numpy pandas matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning for Molecular Modeling\n",
    "\n",
    "In this lab, we'll explore basic machine learning concepts using the scikit-learn library. We will cover:\n",
    "\n",
    "* [Data Preprocessing](#data-preprocessing)\n",
    "* [Regression](#regression)\n",
    "    * [Linear Regression](#linear-regression)\n",
    "    * [Decision Tree Regression](#decision-tree-regression)\n",
    "    * [Random Forest Regression](#random-forest-regression)\n",
    "    * [Support Vector Regression (SVR)](#support-vector-regression-svr)\n",
    "* [Classification](#classification)\n",
    "    * [Logistic Regression](#logistic-regression)\n",
    "    * [Decision Tree Classification](#decision-tree-classification)\n",
    "    * [Random Forest Classification](#random-forest-classification)\n",
    "    * [Support Vector Classification (SVC)](#support-vector-classification-svc)\n",
    "* [Unsupervised Learning](#unsupervised-learning)\n",
    "    * [Principal Component Analysis (PCA)](#principal-component-analysis-pca)\n",
    "    * [K-Means Clustering](#k-means-clustering)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "We will use RDKit to convert the SMILES strings in our dataset to molecular fingerprints, which will serve as features for our machine learning models. We'll also split the data into training and testing sets and standardize the features for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdFingerprintGenerator\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('data/curated_solubility_dataset.csv')\n",
    "\n",
    "# Convert Mols to molecular fingerprints\n",
    "mfpgen = rdFingerprintGenerator.GetMorganGenerator(radius=3, fpSize=2048)\n",
    "def mols_to_fingerprints(mols, fp_gen=mfpgen):\n",
    "    feature_vectors = []\n",
    "    for mol in mols:\n",
    "        fp = mfpgen.GetFingerprint(mol)\n",
    "        feature_vectors.append(fp)\n",
    "    return np.array(feature_vectors)\n",
    "\n",
    "df['Fingerprint'] = mols_to_fingerprints([Chem.MolFromSmiles(smi) for smi in df['SMILES']]).tolist()\n",
    "\n",
    "# Prepare features (X) and target (y) for regression and classification\n",
    "X = np.array(df['Fingerprint'].tolist())\n",
    "y_regression = df['LogS']\n",
    "y_classification = np.where(df['LogS'] < -3, 1, 0)  # Binary classification (-3 threshold)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train_reg, y_test_reg = train_test_split(X, y_regression, test_size=0.2, random_state=42)\n",
    "X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(X, y_classification, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of the target variable\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "sns.histplot(y_regression, kde=True, ax=ax[0])\n",
    "ax[0].set_title('LogS Distribution')\n",
    "sns.histplot(y_classification, kde=False, ax=ax[1])\n",
    "ax[1].set_title('Binary Classification Distribution')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "In regression, we predict continuous values, such as solubility (LogS). To evaluate the performance of our regression models, we use the following metrics:\n",
    "\n",
    "* R² (Coefficient of Determination): Measures how well the model explains the variance in the target variable. The closer to 1, the better.\n",
    "* Mean Squared Error (MSE): The average squared difference between predicted and true values.\n",
    "* Mean Absolute Error (MAE): The average absolute difference between predicted and true values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "Linear Regression is a simple method that fits a straight line to the data. It minimizes the difference between the predicted and actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "linear_reg = LinearRegression()\n",
    "linear_reg.fit(X_train_scaled, y_train_reg)\n",
    "y_pred_lr = linear_reg.predict(X_test_scaled)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "print(\"Linear Regression R²:\", r2_score(y_test_reg, y_pred_lr))\n",
    "print(\"Linear Regression MSE:\", mean_squared_error(y_test_reg, y_pred_lr))\n",
    "print(\"Linear Regression MAE:\", mean_absolute_error(y_test_reg, y_pred_lr))\n",
    "\n",
    "# Plot the predicted vs actual values\n",
    "plt.scatter(y_test_reg, y_pred_lr, color='blue', alpha=0.5)\n",
    "plt.xlabel('True LogS')\n",
    "plt.ylabel('Predicted LogS')\n",
    "plt.title('Linear Regression: Predicted vs True LogS')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature importance\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "feature_importance = np.abs(linear_reg.coef_)\n",
    "\n",
    "# make importances relative to max importance\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "\n",
    "# only show top 10 features\n",
    "sorted_idx = sorted_idx[-10:]\n",
    "pos = pos[-10:]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.xticks(pos, sorted_idx)\n",
    "plt.ylabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Regression\n",
    "\n",
    "Decision Trees partition the data by making splits based on the features. Each split attempts to reduce the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Regression\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(random_state=42)\n",
    "tree_reg.fit(X_train_scaled, y_train_reg)\n",
    "y_pred_tree = tree_reg.predict(X_test_scaled)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "print(\"Decision Tree Regression R²:\", r2_score(y_test_reg, y_pred_tree))\n",
    "print(\"Decision Tree Regression MSE:\", mean_squared_error(y_test_reg, y_pred_tree))\n",
    "print(\"Decision Tree Regression MAE:\", mean_absolute_error(y_test_reg, y_pred_tree))\n",
    "\n",
    "# Plot the predicted vs actual values\n",
    "plt.scatter(y_test_reg, y_pred_tree, color='green', alpha=0.5)\n",
    "plt.xlabel('True LogS')\n",
    "plt.ylabel('Predicted LogS')\n",
    "plt.title('Decision Tree Regression: Predicted vs True LogS')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot tree structure\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(tree_reg, filled=True, feature_names=np.arange(2048).tolist(), max_depth=3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature importance\n",
    "feature_importance = tree_reg.feature_importances_\n",
    "\n",
    "# make importances relative to max importance\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "\n",
    "# only show top 10 features\n",
    "sorted_idx = sorted_idx[-10:]\n",
    "pos = pos[-10:]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.xticks(pos, sorted_idx)\n",
    "plt.ylabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regression\n",
    "\n",
    "Random Forests are an ensemble of decision trees. Instead of relying on a single tree, multiple trees are built, and the results are averaged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_reg = RandomForestRegressor(random_state=42)\n",
    "rf_reg.fit(X_train_scaled, y_train_reg)\n",
    "y_pred_rf = rf_reg.predict(X_test_scaled)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "print(\"Random Forest Regression R²:\", r2_score(y_test_reg, y_pred_rf))\n",
    "print(\"Random Forest Regression MSE:\", mean_squared_error(y_test_reg, y_pred_rf))\n",
    "print(\"Random Forest Regression MAE:\", mean_absolute_error(y_test_reg, y_pred_rf))\n",
    "\n",
    "# Plot the predicted vs actual values\n",
    "plt.scatter(y_test_reg, y_pred_rf, color='red', alpha=0.5)\n",
    "plt.xlabel('True LogS')\n",
    "plt.ylabel('Predicted LogS')\n",
    "plt.title('Random Forest Regression: Predicted vs True LogS')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Regression (SVR)\n",
    "\n",
    "Support Vector Regression fits a hyperplane between the data points, minimizing the error by ensuring most of the predictions fall within a defined margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Regression\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "svm_reg = SVR()\n",
    "svm_reg.fit(X_train_scaled, y_train_reg)\n",
    "y_pred_svm = svm_reg.predict(X_test_scaled)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "print(\"SVM Regression R²:\", r2_score(y_test_reg, y_pred_svm))\n",
    "print(\"SVM Regression MSE:\", mean_squared_error(y_test_reg, y_pred_svm))\n",
    "print(\"SVM Regression MAE:\", mean_absolute_error(y_test_reg, y_pred_svm))\n",
    "\n",
    "# Plot the predicted vs actual values\n",
    "plt.scatter(y_test_reg, y_pred_svm, color='purple', alpha=0.5)\n",
    "plt.xlabel('True LogS')\n",
    "plt.ylabel('Predicted LogS')\n",
    "plt.title('SVM Regression: Predicted vs True LogS')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Models\n",
    "\n",
    "In classification, we predict a discrete label (e.g., whether a molecule is soluble or insoluble). To evaluate classification models, we use:\n",
    "\n",
    "* Precision: The proportion of positive identifications that were actually correct.\n",
    "* Recall: The proportion of actual positives that were identified correctly.\n",
    "* F1 Score: The harmonic mean of precision and recall.\n",
    "* Confusion Matrix: A table showing the number of true positives, false positives, true negatives, and false negatives.\n",
    "* Accuracy: The overall rate of correct predictions.\n",
    "* A confusion matrix visualizes the performance of the classifier by showing how many predictions fall into each category (true positives, false negatives, etc.).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Logistic Regression is a linear classifier that calculates the probability of a binary outcome based on the input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, roc_auc_score\n",
    "import seaborn as sns\n",
    "\n",
    "logistic_clf = LogisticRegression(random_state=42, max_iter=1000)\n",
    "logistic_clf.fit(X_train_class, y_train_class)\n",
    "y_pred_log = logistic_clf.predict(X_test_class)\n",
    "\n",
    "# Calculate Accuracy\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test_class, y_pred_log))\n",
    "\n",
    "# Classification Report (Precision, Recall, F1-Score)\n",
    "print(classification_report(y_test_class, y_pred_log))\n",
    "\n",
    "# Confusion Matrix and ROC Curve\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Confusion Matrix as Heatmap\n",
    "conf_matrix = confusion_matrix(y_test_class, y_pred_log)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', ax=ax[0])\n",
    "ax[0].set_title('Logistic Regression Confusion Matrix')\n",
    "ax[0].set_ylabel('True Label')\n",
    "ax[0].set_xlabel('Predicted Label')\n",
    "\n",
    "# ROC Curve\n",
    "y_pred_log_prob = logistic_clf.predict_proba(X_test_class)[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_test_class, y_pred_log_prob)\n",
    "roc_auc = roc_auc_score(y_test_class, y_pred_log_prob)\n",
    "ax[1].plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "ax[1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "ax[1].set_xlim([0.0, 1.0])\n",
    "ax[1].set_ylim([0.0, 1.05])\n",
    "ax[1].set_xlabel('False Positive Rate')\n",
    "ax[1].set_ylabel('True Positive Rate')\n",
    "ax[1].set_title('Receiver Operating Characteristic')\n",
    "ax[1].legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classification\n",
    "\n",
    "Decision Trees for classification split the data into different classes based on the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Classification\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf.fit(X_train_class, y_train_class)\n",
    "y_pred_tree_clf = tree_clf.predict(X_test_class)\n",
    "\n",
    "# Calculate Accuracy\n",
    "print(\"Decision Tree Classifier Accuracy:\", accuracy_score(y_test_class, y_pred_tree_clf))\n",
    "\n",
    "# Classification Report (Precision, Recall, F1-Score)\n",
    "print(classification_report(y_test_class, y_pred_tree_clf))\n",
    "\n",
    "# Confusion Matrix and ROC Curve\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Confusion Matrix as Heatmap\n",
    "conf_matrix = confusion_matrix(y_test_class, y_pred_tree_clf)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Greens', ax=ax[0])\n",
    "ax[0].set_title('Decision Tree Classifier Confusion Matrix')\n",
    "ax[0].set_ylabel('True Label')\n",
    "ax[0].set_xlabel('Predicted Label')\n",
    "\n",
    "# ROC Curve\n",
    "y_pred_tree_prob = tree_clf.predict_proba(X_test_class)[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_test_class, y_pred_tree_prob)\n",
    "roc_auc = roc_auc_score(y_test_class, y_pred_tree_prob)\n",
    "ax[1].plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "ax[1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "ax[1].set_xlim([0.0, 1.0])\n",
    "ax[1].set_ylim([0.0, 1.05])\n",
    "ax[1].set_xlabel('False Positive Rate')\n",
    "ax[1].set_ylabel('True Positive Rate')\n",
    "ax[1].set_title('Receiver Operating Characteristic')\n",
    "ax[1].legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot tree structure\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(tree_clf, filled=True, feature_names=np.arange(2048).tolist(), max_depth=3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classification\n",
    "\n",
    "Random Forests build multiple decision trees and average the results for classification, reducing overfitting and improving accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "rf_clf.fit(X_train_class, y_train_class)\n",
    "y_pred_rf_clf = rf_clf.predict(X_test_class)\n",
    "\n",
    "# Calculate Accuracy\n",
    "print(\"Random Forest Classifier Accuracy:\", accuracy_score(y_test_class, y_pred_rf_clf))\n",
    "\n",
    "# Classification Report (Precision, Recall, F1-Score)\n",
    "print(classification_report(y_test_class, y_pred_rf_clf))\n",
    "\n",
    "# Confusion Matrix and ROC Curve\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Confusion Matrix as Heatmap\n",
    "conf_matrix = confusion_matrix(y_test_class, y_pred_rf_clf)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Reds', ax=ax[0])\n",
    "ax[0].set_title('Random Forest Classifier Confusion Matrix')\n",
    "ax[0].set_ylabel('True Label')\n",
    "ax[0].set_xlabel('Predicted Label')\n",
    "\n",
    "# ROC Curve\n",
    "y_pred_rf_prob = rf_clf.predict_proba(X_test_class)[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_test_class, y_pred_rf_prob)\n",
    "roc_auc = roc_auc_score(y_test_class, y_pred_rf_prob)\n",
    "ax[1].plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "ax[1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "ax[1].set_xlim([0.0, 1.0])\n",
    "ax[1].set_ylim([0.0, 1.05])\n",
    "ax[1].set_xlabel('False Positive Rate')\n",
    "ax[1].set_ylabel('True Positive Rate')\n",
    "ax[1].set_title('Receiver Operating Characteristic')\n",
    "ax[1].legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Classification (SVC)\n",
    "\n",
    "Support Vector Machines attempt to find the optimal boundary between classes that maximizes the margin between the points of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Classification\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_clf = SVC()\n",
    "svm_clf.fit(X_train_class, y_train_class)\n",
    "y_pred_svm_clf = svm_clf.predict(X_test_class)\n",
    "\n",
    "# Calculate Accuracy\n",
    "print(\"SVM Classifier Accuracy:\", accuracy_score(y_test_class, y_pred_svm_clf))\n",
    "\n",
    "# Classification Report (Precision, Recall, F1-Score)\n",
    "print(classification_report(y_test_class, y_pred_svm_clf))\n",
    "\n",
    "# Confusion Matrix and ROC Curve\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Confusion Matrix as Heatmap\n",
    "conf_matrix = confusion_matrix(y_test_class, y_pred_svm_clf)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Purples', ax=ax[0])\n",
    "ax[0].set_title('SVM Classifier Confusion Matrix')\n",
    "ax[0].set_ylabel('True Label')\n",
    "ax[0].set_xlabel('Predicted Label')\n",
    "\n",
    "# ROC Curve\n",
    "y_pred_svm_prob = svm_clf.decision_function(X_test_class)\n",
    "fpr, tpr, _ = roc_curve(y_test_class, y_pred_svm_prob)\n",
    "roc_auc = roc_auc_score(y_test_class, y_pred_svm_prob)\n",
    "ax[1].plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "ax[1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "ax[1].set_xlim([0.0, 1.0])\n",
    "ax[1].set_ylim([0.0, 1.05])\n",
    "ax[1].set_xlabel('False Positive Rate')\n",
    "ax[1].set_ylabel('True Positive Rate')\n",
    "ax[1].set_title('Receiver Operating Characteristic')\n",
    "ax[1].legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning\n",
    "\n",
    "In this section, we will apply Principal Component Analysis (PCA) and K-means Clustering to explore the dataset.\n",
    "\n",
    "### Principal Component Analysis (PCA)\n",
    "\n",
    "PCA reduces the dimensionality of the dataset while retaining most of the variance in the data. This allows us to visualize high-dimensional data in a 2D space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA to the standardized training data\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "# Scatter plot of the first two principal components\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_train_reg, cmap='viridis', alpha=0.3)\n",
    "plt.title('PCA of Molecular Fingerprints')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.colorbar(label='LogS')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means Clustering\n",
    "\n",
    "K-means Clustering attempts to group data points into distinct clusters based on their features. We'll apply K-means to the PCA-reduced data to find natural clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Apply K-means clustering\n",
    "kmeans = KMeans(n_clusters=7, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(X_pca)\n",
    "\n",
    "# Scatter plot of the clusters\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap='viridis', alpha=0.7)\n",
    "plt.title('K-means Clustering of Molecular Fingerprints')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biopy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
