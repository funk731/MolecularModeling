{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eneskelestemur/MolecularModeling/blob/main/Labs/lab06_deep_learning/Deep_learning.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required packages\n",
    "%pip install numpy pandas matplotlib keras tensorflow scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Molecular Modeling\n",
    "\n",
    "In this lab, we will explore fundamental deep learning concepts using the Keras library. We will cover:\n",
    "\n",
    "* [Data Preprocessing](#data-preprocessing)\n",
    "* [Basic Neural Networks](#basic-neural-networks)\n",
    "* [1D Convolutional Neural Networks (CNNs) for Text Data](#1d-convolutional-neural-networks-cnns-for-text-data)\n",
    "* [Overfitting vs Underfitting](#overfitting-vs-underfitting)\n",
    "* [Regularization Methods](#regularization-methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Before we can build and train a deep learning model, we need to preprocess the data. This involves scaling the data, splitting it into training and testing sets, and converting it into a format suitable for neural networks. We will use the scikit-learn library for scaling and splitting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdFingerprintGenerator\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('data/curated_solubility_dataset.csv')\n",
    "\n",
    "# to make the code faster, let's use only a subset of the data\n",
    "df = df.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# Convert Mols to molecular fingerprints\n",
    "mfpgen = rdFingerprintGenerator.GetMorganGenerator(radius=3, fpSize=2048)\n",
    "def mols_to_fingerprints(mols, fp_gen=mfpgen, counts=False):\n",
    "    feature_vectors = []\n",
    "    for mol in mols:\n",
    "        if counts:\n",
    "            fp = fp_gen.GetCountFingerprint(mol)\n",
    "        else:\n",
    "            fp = fp_gen.GetFingerprint(mol)\n",
    "        feature_vectors.append(fp)\n",
    "    return np.array(feature_vectors)\n",
    "\n",
    "df['Fingerprint'] = mols_to_fingerprints([Chem.MolFromSmiles(smi) for smi in df['SMILES']]).tolist()\n",
    "\n",
    "# Prepare features (X) and target (y) for regression and classification\n",
    "X = np.array(df['Fingerprint'].tolist())\n",
    "X_smiles = df['SMILES'].values\n",
    "y_regression = df['LogS']\n",
    "y_classification = np.where(df['LogS'] < -3, 1, 0)  # Binary classification (-3 threshold)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X, y_regression, test_size=0.2, random_state=42)\n",
    "X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(X, y_classification, test_size=0.2, random_state=42)\n",
    "X_train_smiles, X_test_smiles, y_train_smiles, y_test_smiles = train_test_split(X_smiles, y_regression, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Neural Networks\n",
    "\n",
    "A neural network consists of layers of neurons, where each neuron performs a linear combination of inputs, followed by a non-linear activation function. We'll build a simple neural network for regression and classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "# Building a simple neural network for regression\n",
    "reg_model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train_reg.shape[1],)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1)  # Regression output\n",
    "])\n",
    "\n",
    "# Compiling the regression model\n",
    "reg_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Training the regression model\n",
    "history_reg = reg_model.fit(X_train_reg, y_train_reg, epochs=10, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a simple neural network for classification\n",
    "class_model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train_class.shape[1],)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')  # Binary classification output\n",
    "])\n",
    "\n",
    "# Compiling the classification model\n",
    "class_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the classification model\n",
    "history_class = class_model.fit(X_train_class, y_train_class, epochs=10, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D Convolutional Neural Networks (CNNs) for SMILES (Text Data)\n",
    "\n",
    "SMILES strings, which represent the structure of chemical compounds, can be treated as text data. We will use a 1D CNN to classify these SMILES strings by first applying TextVectorization and embedding the SMILES into a vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the SMILES strings\n",
    "max_len = max(df['SMILES'].apply(len))\n",
    "vectorizer = layers.TextVectorization(max_tokens=5000, output_sequence_length=max_len)\n",
    "vectorizer.adapt(df['SMILES'].values)\n",
    "\n",
    "# Build a 1D CNN for SMILES regression\n",
    "smiles_model = keras.Sequential([\n",
    "    vectorizer,\n",
    "    layers.Embedding(input_dim=5000, output_dim=50, input_length=max_len),\n",
    "    layers.Conv1D(128, kernel_size=5, activation='relu'),\n",
    "    layers.GlobalMaxPooling1D(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1)  # Regression output\n",
    "])\n",
    "\n",
    "# Compile the 1D CNN model\n",
    "smiles_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Train the model using SMILES data for classification\n",
    "history_smiles = smiles_model.fit(X_train_smiles, y_train_smiles, epochs=10, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting vs Underfitting\n",
    "\n",
    "When training a deep learning model, we need to ensure that the model generalizes well to unseen data. Overfitting occurs when the model performs well on the training data but poorly on new, unseen data. Underfitting occurs when the model performs poorly on both the training data and new data, indicating it hasn't learned the underlying patterns.\n",
    "\n",
    "### Regularization Methods\n",
    "\n",
    "To combat overfitting, we can apply regularization techniques:\n",
    "\n",
    "* L1 Regularization: Adds a penalty equal to the absolute value of the coefficients, encouraging sparsity.\n",
    "* L2 Regularization: Adds a penalty equal to the square of the coefficients, discouraging large weights.\n",
    "* Dropout: Randomly sets a fraction of input units to 0 at each update during training, preventing the model from becoming too reliant on any specific neuron.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding L2 Regularization and Dropout to a neural network\n",
    "regularized_model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train_reg.shape[1],), \n",
    "                 kernel_regularizer=keras.regularizers.L2(0.001)),\n",
    "    layers.Dropout(0.5),  # Dropout with 50% rate\n",
    "    layers.Dense(32, activation='relu', kernel_regularizer=keras.regularizers.L1(0.001)),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1)  # Regression output\n",
    "])\n",
    "\n",
    "# Compile the regularized model\n",
    "regularized_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Train the regularized model\n",
    "history_regularized = regularized_model.fit(X_train_reg, y_train_reg, epochs=10, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation and Plotting Training History\n",
    "\n",
    "Once the model is trained, we evaluate its performance on the test set and visualize the training history for both regression and classification tasks.\n",
    "\n",
    "* Loss Plot: For both regression and classification models, the loss (both training and validation) is plotted to visualize overfitting or underfitting behavior.\n",
    "* Accuracy Plot: For the classification models, accuracy (both training and validation) is plotted to observe performance improvements over epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses and metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Regression models\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_reg.history['loss'], label='Training Loss')\n",
    "plt.plot(history_reg.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Regression Model Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_reg.history['mean_absolute_error'], label='Training MAE')\n",
    "plt.plot(history_reg.history['val_mean_absolute_error'], label='Validation MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.title('Regression Model MAE')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Classification models\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_class.history['loss'], label='Training Loss')\n",
    "plt.plot(history_class.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (Binary Crossentropy)')\n",
    "plt.title('Classification Model Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_class.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history_class.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Classification Model Accuracy')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# SMILES regression model\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_smiles.history['loss'], label='Training Loss')\n",
    "plt.plot(history_smiles.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('SMILES Regression Model Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_smiles.history['mean_absolute_error'], label='Training MAE')\n",
    "plt.plot(history_smiles.history['val_mean_absolute_error'], label='Validation MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.title('SMILES Regression Model MAE')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "## test set evaluation\n",
    "# Regression model: scatter plot of predicted vs. true values\n",
    "y_pred_reg = reg_model.predict(X_test_reg).flatten()\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_test_reg, y_pred_reg, alpha=0.5)\n",
    "plt.annotate(f'R2 Score: {r2_score(y_test_reg, y_pred_reg):.2f}', xy=(0.05, 0.9), xycoords='axes fraction')\n",
    "plt.xlabel('True LogS')\n",
    "plt.ylabel('Predicted LogS')\n",
    "plt.title('Regression Model: True vs. Predicted LogS')\n",
    "plt.show()\n",
    "\n",
    "# Classification model: confusion matrix\n",
    "y_pred_class = (class_model.predict(X_test_class) > 0.5).flatten()\n",
    "cm = confusion_matrix(y_test_class, y_pred_class)\n",
    "plt.imshow(cm, cmap='Blues', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Classification Model: Confusion Matrix')\n",
    "plt.xticks([0, 1], ['Soluble', 'Insoluble'])\n",
    "plt.yticks([0, 1], ['Soluble', 'Insoluble'])\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j, i, cm[i, j], ha='center', va='center')\n",
    "plt.show()\n",
    "\n",
    "# SMILES regression model: scatter plot of predicted vs. true values\n",
    "y_pred_smiles = smiles_model.predict(X_test_smiles).flatten()\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_test_smiles, y_pred_smiles, alpha=0.5)\n",
    "plt.annotate(f'R2 Score: {r2_score(y_test_smiles, y_pred_smiles):.2f}', xy=(0.05, 0.9), xycoords='axes fraction')\n",
    "plt.xlabel('True LogS')\n",
    "plt.ylabel('Predicted LogS')\n",
    "plt.title('SMILES Regression Model: True vs. Predicted LogS')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
